{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ebc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiktionary lookup:  13%|â–ˆâ–ˆâ–ˆâ–Ž                     | 4/30 [00:05<00:32,  1.25s/it]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sanderputs/git/tests/pymusas_translate/wiktionary_lookup.py\", line 217, in <module>\n",
      "    main()\n",
      "  File \"/Users/sanderputs/git/tests/pymusas_translate/wiktionary_lookup.py\", line 198, in main\n",
      "    url, definition = derive_url_and_definition(phrase)\n",
      "  File \"/Users/sanderputs/git/tests/pymusas_translate/wiktionary_lookup.py\", line 151, in derive_url_and_definition\n",
      "    definition = definition_from_title(guess)\n",
      "  File \"/Users/sanderputs/git/tests/pymusas_translate/wiktionary_lookup.py\", line 122, in definition_from_title\n",
      "    wt = revisions_request(title)\n",
      "  File \"/Users/sanderputs/git/tests/pymusas_translate/wiktionary_lookup.py\", line 94, in revisions_request\n",
      "    res = requests.get(API, params=params, timeout=10, headers={\"User-Agent\": USER_AGENT}).json()\n",
      "  File \"/Users/sanderputs/git/tests/.venv/lib/python3.13/site-packages/requests/api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"/Users/sanderputs/git/tests/.venv/lib/python3.13/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/sanderputs/git/tests/.venv/lib/python3.13/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/sanderputs/git/tests/.venv/lib/python3.13/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/sanderputs/git/tests/.venv/lib/python3.13/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/sanderputs/git/tests/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/Users/sanderputs/git/tests/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "  File \"/Users/sanderputs/git/tests/.venv/lib/python3.13/site-packages/urllib3/connection.py\", line 516, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 1430, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 331, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py\", line 292, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py\", line 1304, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py\", line 1138, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python wiktionary_lookup.py --input \"/Users/sanderputs/git/tests/mwe_en_cleaned.xlsx\" --sample 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiktionary lookup:   0%|                        | 1/200 [00:01<03:20,  1.01s/it]^C\n"
     ]
    }
   ],
   "source": [
    "!python wiktionary_lookup.py --input \"/Users/sanderputs/git/tests/mwe_en_cleaned_wiktionary.csv\" --sample 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Filtering to 77 rows where wiktionary_sanity_check=False\n",
      "Wikipedia lookup: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [02:10<00:00,  1.69s/it]\n",
      "âœ… Results written to /Users/sanderputs/git/tests/mwe_en_cleaned_wiktionary_200_wikipedia_all_filtered.csv\n",
      "ðŸ“Š Output contains 77 processed rows and 123 unprocessed rows\n"
     ]
    }
   ],
   "source": [
    "!python wikipedia_lookup.py --input \"/Users/sanderputs/git/tests/mwe_en_cleaned_wiktionary.csv\" --sample 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ae7b7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Wikipedia Sanity Check Statistics\n",
      "========================================\n",
      "Value counts:\n",
      "  nan: 123 (61.5%)\n",
      "  False: 48 (24.0%)\n",
      "  True: 29 (14.5%)\n",
      "\n",
      "Total rows: 200\n",
      "\n",
      "ðŸ“Š Cross-tabulation: Wiktionary vs Wikipedia Sanity Checks\n",
      "============================================================\n",
      "wikipedia_sanity_check   False  True  NaN  All\n",
      "wiktionary_sanity_check                       \n",
      "False                       48    29    0   77\n",
      "True                         0     0  123  123\n",
      "All                         48    29    0  200\n",
      "\n",
      "ðŸ“Š Success Rates\n",
      "====================\n",
      "Wiktionary success rate: 123/200 (61.5%)\n",
      "Wikipedia success rate: 29/77 (37.7%)\n",
      "Combined success rate: 152/200 (76.0%)\n",
      "\n",
      "ðŸ” Sample of failed lookups (both Wiktionary and Wikipedia failed):\n",
      "======================================================================\n",
      " 1. mortgag* * up to the * hilt\n",
      " 2. Nobel Prize-winner*\n",
      " 3. recall audition*\n",
      " 4. special educational difficulties\n",
      " 5. construction studies\n",
      " 6. New Town Aston\n",
      " 7. Antique Worlds\n",
      " 8. * and * ago\n",
      " 9. wiring * across\n",
      "10. drag* * away\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the output file\n",
    "df = pd.read_csv(\"/Users/sanderputs/git/tests/mwe_en_cleaned_wiktionary_200_wikipedia_all_filtered.csv\")\n",
    "\n",
    "# Calculate stats on wikipedia_sanity_check column\n",
    "print(\"ðŸ“Š Wikipedia Sanity Check Statistics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Basic value counts\n",
    "wiki_check_counts = df['wikipedia_sanity_check'].value_counts(dropna=False)\n",
    "print(f\"Value counts:\")\n",
    "for value, count in wiki_check_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  {value}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal rows: {len(df)}\")\n",
    "\n",
    "# Compare with wiktionary_sanity_check\n",
    "print(\"\\nðŸ“Š Cross-tabulation: Wiktionary vs Wikipedia Sanity Checks\")\n",
    "print(\"=\" * 60)\n",
    "crosstab = pd.crosstab(\n",
    "    df['wiktionary_sanity_check'], \n",
    "    df['wikipedia_sanity_check'], \n",
    "    margins=True, \n",
    "    dropna=False\n",
    ")\n",
    "print(crosstab)\n",
    "\n",
    "# Success rates\n",
    "print(\"\\nðŸ“Š Success Rates\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Wiktionary success rate\n",
    "wikt_success = df['wiktionary_sanity_check'].value_counts().get(True, 0)\n",
    "wikt_total = df['wiktionary_sanity_check'].notna().sum()\n",
    "print(f\"Wiktionary success rate: {wikt_success}/{wikt_total} ({(wikt_success/wikt_total)*100:.1f}%)\")\n",
    "\n",
    "# Wikipedia success rate (only for processed rows)\n",
    "wiki_processed = df['wikipedia_sanity_check'].notna()\n",
    "wiki_success = df[wiki_processed]['wikipedia_sanity_check'].value_counts().get(True, 0)\n",
    "wiki_total = wiki_processed.sum()\n",
    "print(f\"Wikipedia success rate: {wiki_success}/{wiki_total} ({(wiki_success/wiki_total)*100:.1f}%)\")\n",
    "\n",
    "# Combined success rate (either Wiktionary OR Wikipedia succeeded)\n",
    "either_success = ((df['wiktionary_sanity_check'] == True) | \n",
    "                  (df['wikipedia_sanity_check'] == True)).sum()\n",
    "print(f\"Combined success rate: {either_success}/{len(df)} ({(either_success/len(df))*100:.1f}%)\")\n",
    "\n",
    "# Show some examples of failed lookups\n",
    "print(\"\\nðŸ” Sample of failed lookups (both Wiktionary and Wikipedia failed):\")\n",
    "print(\"=\" * 70)\n",
    "failed_both = df[\n",
    "    (df['wiktionary_sanity_check'] == False) & \n",
    "    (df['wikipedia_sanity_check'] == False)\n",
    "]['pos_cleaned'].head(10)\n",
    "\n",
    "for i, phrase in enumerate(failed_both, 1):\n",
    "    print(f\"{i:2d}. {phrase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87edbb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Filtering to 48 rows using filter mode 'both-false'\n",
      "WordNet lookup: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:15<00:00,  3.02it/s]\n",
      "âœ… Results written to /Users/sanderputs/git/tests/mwe_en_cleaned_wiktionary_200_wikipedia_all_filtered_wordnet_all_both-false.csv\n",
      "ðŸ“Š Processed 48 rows, found 0 WordNet matches (0.0%)\n",
      "ðŸ“Š Output contains 48 processed rows and 152 unprocessed rows\n"
     ]
    }
   ],
   "source": [
    "!python wordnet_lookup.py --input \"/Users/sanderputs/git/tests/mwe_en_cleaned_wiktionary_200_wikipedia_all_filtered.csv\" --filter-mode both-false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ebec69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Detailed Analysis of Failed Lookups\n",
      "==================================================\n",
      "ðŸ“Š Failure Categories\n",
      "------------------------------\n",
      "success        : 152 (76.0%)\n",
      "all_failed     :  48 (24.0%)\n",
      "\n",
      "ðŸ“Š Complete Failures (all 3 sources failed)\n",
      "---------------------------------------------\n",
      "Total complete failures: 48\n",
      "Average word count: 3.3\n",
      "Length range: 2-8 words\n",
      "\n",
      "ðŸ” Pattern Analysis of Failed Phrases:\n",
      "----------------------------------------\n",
      "Contains wildcards (*): 24/48 (50.0%)\n",
      "Contains hyphens (-): 1/48 (2.1%)\n",
      "Contains numbers: 1/48 (2.1%)\n",
      "Contains special chars: 0/48 (0.0%)\n",
      "\n",
      "ðŸŽ¯ Examples of Complete Failures (first 15):\n",
      "---------------------------------------------\n",
      " 1. mortgag* * up to the * hilt    (7w) [*]\n",
      " 2. Nobel Prize-winner*            (2w) [*,-]\n",
      " 3. recall audition*               (2w) [*]\n",
      " 4. special educational difficulties (3w) \n",
      " 5. construction studies           (2w) \n",
      " 6. New Town Aston                 (3w) \n",
      " 7. Antique Worlds                 (2w) \n",
      " 8. * and * ago                    (4w) [*]\n",
      " 9. wiring * across                (3w) [*]\n",
      "10. drag* * away                   (3w) [*]\n",
      "11. Hinge and Brackett             (3w) \n",
      "12. Calmer Classical               (2w) \n",
      "13. J R Tolkein                    (3w) \n",
      "14. all round the world            (4w) \n",
      "15. cross examin*                  (2w) [*]\n",
      "\n",
      "ðŸŽ¯ Successful Rescues by Source:\n",
      "-----------------------------------\n",
      "\n",
      "ðŸ“Š Success Matrix\n",
      "-------------------------\n",
      "Source     | Success | Unique\n",
      "-------------------------\n",
      "Wiktionary |     123 |      0\n",
      "Wikipedia  |      29 |      0\n",
      "WordNet    |       0 |      0\n",
      "-------------------------\n",
      "All Failed |      48 |\n"
     ]
    }
   ],
   "source": [
    "# Load the WordNet results\n",
    "df_wordnet = pd.read_csv(\"/Users/sanderputs/git/tests/mwe_en_cleaned_wiktionary_200_wikipedia_all_filtered_wordnet_all_both-false.csv\")\n",
    "\n",
    "print(\"ðŸ“Š Detailed Analysis of Failed Lookups\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create failure categories\n",
    "df_wordnet['failure_category'] = 'success'\n",
    "\n",
    "# All three failed\n",
    "all_failed = (df_wordnet['wiktionary_sanity_check'] == False) & \\\n",
    "             (df_wordnet['wikipedia_sanity_check'] == False) & \\\n",
    "             (df_wordnet['wordnet_sanity_check'] == False)\n",
    "\n",
    "# Two failed, one succeeded\n",
    "wikt_only = (df_wordnet['wiktionary_sanity_check'] == True) & \\\n",
    "            (df_wordnet['wikipedia_sanity_check'] == False) & \\\n",
    "            (df_wordnet['wordnet_sanity_check'] == False)\n",
    "\n",
    "wiki_only = (df_wordnet['wiktionary_sanity_check'] == False) & \\\n",
    "            (df_wordnet['wikipedia_sanity_check'] == True) & \\\n",
    "            (df_wordnet['wordnet_sanity_check'] == False)\n",
    "\n",
    "wordnet_only = (df_wordnet['wiktionary_sanity_check'] == False) & \\\n",
    "               (df_wordnet['wikipedia_sanity_check'] == False) & \\\n",
    "               (df_wordnet['wordnet_sanity_check'] == True)\n",
    "\n",
    "# Apply categories\n",
    "df_wordnet.loc[all_failed, 'failure_category'] = 'all_failed'\n",
    "df_wordnet.loc[wikt_only, 'failure_category'] = 'wiktionary_only'\n",
    "df_wordnet.loc[wiki_only, 'failure_category'] = 'wikipedia_only'\n",
    "df_wordnet.loc[wordnet_only, 'failure_category'] = 'wordnet_only'\n",
    "\n",
    "# Category counts\n",
    "print(\"ðŸ“Š Failure Categories\")\n",
    "print(\"-\" * 30)\n",
    "category_counts = df_wordnet['failure_category'].value_counts()\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / len(df_wordnet)) * 100\n",
    "    print(f\"{category:15}: {count:3d} ({percentage:4.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Complete Failures (all 3 sources failed)\")\n",
    "print(\"-\" * 45)\n",
    "complete_failures = df_wordnet[all_failed]\n",
    "print(f\"Total complete failures: {len(complete_failures)}\")\n",
    "\n",
    "if len(complete_failures) > 0:\n",
    "    # Analyze patterns in failed phrases\n",
    "    failed_phrases = complete_failures['pos_cleaned'].tolist()\n",
    "    \n",
    "    # Length analysis\n",
    "    lengths = [len(phrase.split()) for phrase in failed_phrases]\n",
    "    print(f\"Average word count: {sum(lengths)/len(lengths):.1f}\")\n",
    "    print(f\"Length range: {min(lengths)}-{max(lengths)} words\")\n",
    "    \n",
    "    # Character analysis\n",
    "    print(f\"\\nðŸ” Pattern Analysis of Failed Phrases:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Wildcards\n",
    "    with_wildcards = [p for p in failed_phrases if '*' in p]\n",
    "    print(f\"Contains wildcards (*): {len(with_wildcards)}/{len(failed_phrases)} ({len(with_wildcards)/len(failed_phrases)*100:.1f}%)\")\n",
    "    \n",
    "    # Hyphens\n",
    "    with_hyphens = [p for p in failed_phrases if '-' in p]\n",
    "    print(f\"Contains hyphens (-): {len(with_hyphens)}/{len(failed_phrases)} ({len(with_hyphens)/len(failed_phrases)*100:.1f}%)\")\n",
    "    \n",
    "    # Numbers\n",
    "    with_numbers = [p for p in failed_phrases if any(c.isdigit() for c in p)]\n",
    "    print(f\"Contains numbers: {len(with_numbers)}/{len(failed_phrases)} ({len(with_numbers)/len(failed_phrases)*100:.1f}%)\")\n",
    "    \n",
    "    # Special characters\n",
    "    import re\n",
    "    with_special = [p for p in failed_phrases if re.search(r'[^\\w\\s\\-\\*]', p)]\n",
    "    print(f\"Contains special chars: {len(with_special)}/{len(failed_phrases)} ({len(with_special)/len(failed_phrases)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Examples of Complete Failures (first 15):\")\n",
    "    print(\"-\" * 45)\n",
    "    for i, phrase in enumerate(failed_phrases[:15], 1):\n",
    "        # Show length and patterns\n",
    "        length = len(phrase.split())\n",
    "        patterns = []\n",
    "        if '*' in phrase: patterns.append('*')\n",
    "        if '-' in phrase: patterns.append('-')\n",
    "        if any(c.isdigit() for c in phrase): patterns.append('num')\n",
    "        if re.search(r'[^\\w\\s\\-\\*]', phrase): patterns.append('special')\n",
    "        \n",
    "        pattern_str = f\"[{','.join(patterns)}]\" if patterns else \"\"\n",
    "        print(f\"{i:2d}. {phrase:30} ({length}w) {pattern_str}\")\n",
    "\n",
    "# Show successful rescues by each source\n",
    "print(f\"\\nðŸŽ¯ Successful Rescues by Source:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "if len(df_wordnet[wordnet_only]) > 0:\n",
    "    print(f\"\\nWordNet-only successes ({len(df_wordnet[wordnet_only])}):\")\n",
    "    for i, phrase in enumerate(df_wordnet[wordnet_only]['pos_cleaned'].head(10), 1):\n",
    "        print(f\"  {i:2d}. {phrase}\")\n",
    "\n",
    "if len(df_wordnet[wiki_only]) > 0:\n",
    "    print(f\"\\nWikipedia-only successes ({len(df_wordnet[wiki_only])}):\")\n",
    "    for i, phrase in enumerate(df_wordnet[wiki_only]['pos_cleaned'].head(10), 1):\n",
    "        print(f\"  {i:2d}. {phrase}\")\n",
    "\n",
    "if len(df_wordnet[wikt_only]) > 0:\n",
    "    print(f\"\\nWiktionary-only successes ({len(df_wordnet[wikt_only])}):\")\n",
    "    for i, phrase in enumerate(df_wordnet[wikt_only]['pos_cleaned'].head(10), 1):\n",
    "        print(f\"  {i:2d}. {phrase}\")\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\nðŸ“Š Success Matrix\")\n",
    "print(\"-\" * 25)\n",
    "print(\"Source     | Success | Unique\")\n",
    "print(\"-\" * 25)\n",
    "wikt_success = (df_wordnet['wiktionary_sanity_check'] == True).sum()\n",
    "wiki_success = (df_wordnet['wikipedia_sanity_check'] == True).sum()\n",
    "wordnet_success = (df_wordnet['wordnet_sanity_check'] == True).sum()\n",
    "\n",
    "print(f\"Wiktionary | {wikt_success:7d} | {len(df_wordnet[wikt_only]):6d}\")\n",
    "print(f\"Wikipedia  | {wiki_success:7d} | {len(df_wordnet[wiki_only]):6d}\")\n",
    "print(f\"WordNet    | {wordnet_success:7d} | {len(df_wordnet[wordnet_only]):6d}\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"All Failed | {len(complete_failures):7d} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b9b070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
