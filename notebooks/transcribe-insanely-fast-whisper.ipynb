{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86dd56b3-a5a8-497c-8d39-8f0f9bb699c9",
   "metadata": {},
   "source": [
    "# Insanely-Fast-Whisper Notebook\n",
    "\n",
    "This repository contains a Jupyter Notebook interface for **speech-to-text (STT)** based on the open-source project **insanely-fast-whisper**:  \n",
    "https://github.com/Vaibhavs10/insanely-fast-whisper\n",
    "\n",
    "The notebook wraps the original command-line tool and enables **high-speed transcription and speaker diarization** of audio recordings, with a particular focus on **Danish interview data**. All processing is performed in a **private cloud environment**, ensuring that sensitive audio data remains within controlled infrastructure and that privacy requirements are preserved.\n",
    "\n",
    "In addition to transcription and diarization, the notebook includes functionality to **export transcripts directly to Microsoft Word (.docx)** format, making the output suitable for analysis, reporting, and archival purposes.\n",
    "\n",
    "## GPU and Memory Requirements\n",
    "\n",
    "With the default configuration, *insanely-fast-whisper* is **GPU-intensive** and requires a **significant amount of GPU memory (VRAM)** to achieve high transcription speed. Systems with limited VRAM may encounter out-of-memory errors when using the default settings.\n",
    "\n",
    "If GPU memory is constrained, the **batch size can be reduced** to lower VRAM consumption, at the cost of reduced throughput. Adjusting the batch size allows the notebook to run on a wider range of GPU hardware while maintaining functional correctness.\n",
    "\n",
    "## Maintenance and Dependency Management\n",
    "\n",
    "The original *insanely-fast-whisper* project is no longer actively maintained, and its dependencies were not pinned to fixed versions. As a result, updates to upstream libraries have frequently caused the project to stop working.\n",
    "\n",
    "To ensure long-term stability, **all dependencies were pinned to known working versions in late 2025**, allowing the tool to continue functioning reliably despite changes in the surrounding software ecosystem.\n",
    "\n",
    "A maintained fork with pinned dependencies is available here:  \n",
    "https://github.com/putssander/insanely-fast-whisper\n",
    "\n",
    "This notebook is designed to work with that forked version and should be used in conjunction with it to ensure reproducible and stable results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aceb9e1-eafa-4c8f-98e1-0bd6014adef1",
   "metadata": {},
   "source": [
    "# Quickrun for ease of use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640e738",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "To use this notebook, you need to set up your Hugging Face token.\n",
    "\n",
    "1.  Create a file named `.env` in the root directory of this project (same level as `notebooks/` folder).\n",
    "2.  Add your Hugging Face token to the `.env` file like this:\n",
    "    ```\n",
    "    HUGGINGFACE_AUTH_TOKEN=your_token_here\n",
    "    ```\n",
    "3.  Alternatively, if you don't create a `.env` file, the notebook will ask you to enter your token interactively when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768cf44-8219-4275-b623-0d3aad2b7e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following 2 files will be transcribed: \n",
      "['/work/speech/Kasper/PrÃ¦test/P4.WMA', '/work/speech/Kasper/PrÃ¦test/P9.WMA']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, re\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "if os.path.exists('.env'):\n",
    "    with open('.env', 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#') and '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                os.environ[key.strip()] = value.strip()\n",
    "\n",
    "# set these 3 variables and run all!\n",
    "# HUGGINGFACE_AUTH_TOKEN is loaded from .env or will be requested interactively later\n",
    "extensions = [\"mp3\", \"WMA\", \"wma\", \"m4a\", \"wav\"]  # List of extensions\n",
    "path_audio = '/work/speech/Kasper/PrÃ¦test/'\n",
    "\n",
    "# path_audio = '/work/speech/Test/single/'\n",
    "\n",
    "# Use glob to match multiple extensions\n",
    "files = []\n",
    "for ext in extensions:\n",
    "    path_audio_regex = f\"{path_audio}*.{ext}\"  # glob does not use regex\n",
    "    files.extend(glob.glob(path_audio_regex))\n",
    "\n",
    "length = len(files)\n",
    "print(f\"The following {length} files will be transcribed: \\n{files}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcae71-1c93-439a-9f49-591d2de94dc3",
   "metadata": {},
   "source": [
    "### 1. Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05ba2a-2054-4d8d-a86f-50b85ba44751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pipx\n",
      "  Downloading pipx-1.8.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting argcomplete>=1.9.4 (from pipx)\n",
      "  Downloading argcomplete-3.6.3-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: packaging>=20 in /opt/conda/lib/python3.12/site-packages (from pipx) (25.0)\n",
      "Requirement already satisfied: platformdirs>=2.1 in /opt/conda/lib/python3.12/site-packages (from pipx) (4.5.0)\n",
      "Collecting userpath!=1.9,>=1.6 (from pipx)\n",
      "  Downloading userpath-1.9.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from userpath!=1.9,>=1.6->pipx) (8.3.1)\n",
      "Downloading pipx-1.8.0-py3-none-any.whl (78 kB)\n",
      "Downloading argcomplete-3.6.3-py3-none-any.whl (43 kB)\n",
      "Downloading userpath-1.9.2-py3-none-any.whl (9.1 kB)\n",
      "Installing collected packages: userpath, argcomplete, pipx\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [pipx]\n",
      "\u001b[1A\u001b[2KSuccessfully installed argcomplete-3.6.3 pipx-1.8.0 userpath-1.9.2\n",
      "\u001b[?25lNothing to uninstall for insanely-fast-whisper ğŸ˜´\n",
      "\u001b[K  installed package \u001b[1minsanely-fast-whisper\u001b[0m \u001b[1m0.0.16\u001b[0m, installed using Python 3.12.12\n",
      "  These apps are now globally available\n",
      "    - insanely-fast-whisper\n",
      "done! âœ¨ ğŸŒŸ âœ¨\n",
      "\u001b[?25h\u001b[0mCollecting python-docx\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting lxml>=3.1.0 (from python-docx)\n",
      "  Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /opt/conda/lib/python3.12/site-packages (from python-docx) (4.15.0)\n",
      "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m197.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml, python-docx\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [python-docx]\u001b[0m [python-docx]\n",
      "\u001b[1A\u001b[2KSuccessfully installed lxml-6.0.2 python-docx-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip pipx\n",
    "!pipx ensurepath\n",
    "\n",
    "# Remove any existing installation\n",
    "!pipx uninstall insanely-fast-whisper || true\n",
    "\n",
    "# Install pinned fork directly from GitHub\n",
    "!pipx install git+https://github.com/putssander/insanely-fast-whisper.git --force\n",
    "\n",
    "# Dependency for Word export\n",
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfa7d9-d314-4456-82b0-6aeb22c1de70",
   "metadata": {},
   "source": [
    "#### 1.B HuggingFace user agreements!\n",
    "To run this notebook without the following is required.\n",
    "- a huggingface account [link](https://huggingface.co/)\n",
    "- a huggingface authentication token [link](https://huggingface.co/settings/tokens)\n",
    "- accept 2 user agreements:\n",
    "  - [https://huggingface.co/pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)\n",
    "  - [https://hf.co/pyannote/segmentation-3.0](https://hf.co/pyannote/segmentation-3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e54b25-7262-4148-a43c-85a1458004ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell, copy paste your huggingface token and press enter\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "if not os.environ.get(\"HUGGINGFACE_AUTH_TOKEN\"):\n",
    "    _set_env(\"HUGGINGFACE_AUTH_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba06fa75-ad62-4370-9790-281b19ef3284",
   "metadata": {},
   "source": [
    "### 2. Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9228aad-2df2-4d02-9c7e-bfe60c3ce4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "LANGUAGE = \"da\"\n",
    "NUM_SPEAKERS = 2\n",
    "TASK = \"transcribe\"\n",
    "DIARIZATION_MODEL = \"pyannote/speaker-diarization-3.1\"\n",
    "HUGGINGFACE_AUTH_TOKEN = os.environ['HUGGINGFACE_AUTH_TOKEN']\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "def transcribe(file_name):\n",
    "    TRANSCRIPT_PATH = os.path.splitext(file_name)[0]+\"-transcription.json\"\n",
    "    FILE_NAME = re.escape(file_name)\n",
    "    TRANSCRIPT_PATH = re.escape(TRANSCRIPT_PATH)    \n",
    "    \n",
    "    # workaround, current MPLBACKEND default does not work\n",
    "    os.environ['MPLBACKEND'] = 'svg'\n",
    "    # !echo $MPLBACKEND\n",
    "    !insanely-fast-whisper --file-name {FILE_NAME} --transcript-path {TRANSCRIPT_PATH} --language {LANGUAGE} --task {TASK} --diarization_model {DIARIZATION_MODEL} --hf-token {HUGGINGFACE_AUTH_TOKEN} --batch-size {BATCH_SIZE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "044c53f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepended to PATH for current kernel session: /home/ucloud/.local/bin:/home/ucloud/.cargo/bin:/opt/go/Projects/Proj1/bin:/usr/local/go/bin:/opt/conda/bin:/home/ucloud/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/spark/bin:/home/ucloud/.local/share/coursier/bin\n"
     ]
    }
   ],
   "source": [
    "# Ensure the kernel process can find pipx-installed CLIs for this session\n",
    "import os, pathlib\n",
    "pipx_bin = str(pathlib.Path.home() / '.local' / 'bin')\n",
    "os.environ['PATH'] = pipx_bin + ':' + os.environ.get('PATH', '')\n",
    "print('Prepended to PATH for current kernel session:', os.environ['PATH'])\n",
    "# After running this cell, `!insanely-fast-whisper` should be available in subsequent cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3e467c-cacc-4f6f-9b49-190028f74b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 1 of 2 file=/work/speech/Kasper/PrÃ¦test/P4.WMA\n",
      "config.json: 1.27kB [00:00, 8.58MB/s]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.09G/3.09G [00:03<00:00, 999MB/s]\n",
      "generation_config.json: 3.90kB [00:00, 33.4MB/s]\n",
      "tokenizer_config.json: 283kB [00:00, 600MB/s]\n",
      "vocab.json: 1.04MB [00:00, 95.4MB/s]\n",
      "tokenizer.json: 2.48MB [00:00, 317MB/s]\n",
      "merges.txt: 494kB [00:00, 190MB/s]\n",
      "normalizer.json: 52.7kB [00:00, 179MB/s]\n",
      "added_tokens.json: 34.6kB [00:00, 172MB/s]\n",
      "special_tokens_map.json: 2.07kB [00:00, 21.5MB/s]\n",
      "preprocessor_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 340/340 [00:00<00:00, 6.79MB/s]\n",
      "Device set to use cuda:0\n",
      "\u001b[2KğŸ¤— \u001b[33mTranscribing...\u001b[0m \u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[33m0:00:00\u001b[0mUsing `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
      "\u001b[2KğŸ¤— \u001b[33mTranscribing...\u001b[0m \u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[33m0:04:49\u001b[0m\n",
      "config.yaml: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:00<00:00, 4.80MB/s]\n",
      "pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.91M/5.91M [00:00<00:00, 9.08MB/s]\n",
      "config.yaml: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 399/399 [00:00<00:00, 5.40MB/s]\n",
      "/home/ucloud/.local/share/pipx/venvs/insanely-fast-whisper/lib/python3.12/site-packages/lightning_fabric/utilities/cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.6M/26.6M [00:00<00:00, 90.3MB/s]\n",
      "config.yaml: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 221/221 [00:00<00:00, 3.81MB/s]\n",
      "\u001b[2K/home/ucloud/.local/share/pipx/venvs/insanely-fast-whisper/lib/python3.12/site-pmâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "ackages/insanely_fast_whisper/utils/diarize.py:55: UserWarning: The given NumPy \n",
      "array is not writable, and PyTorch does not support non-writable tensors. This \n",
      "means writing to this tensor will result in undefined behavior. You may want to \n",
      "copy the array to protect its data or make it writable before converting it to a\n",
      "tensor. This type of warning will be suppressed for the rest of this program. \n",
      "(Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  diarizer_inputs = torch.from_numpy(inputs).float()\n",
      "\u001b[2K/home/ucloud/.local/share/pipx/venvs/insanely-fast-whisper/lib/python3.12/site-p0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "ackages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: \n",
      "TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility \n",
      "issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n",
      "\u001b[2K/home/ucloud/.local/share/pipx/venvs/insanely-fast-whisper/lib/python3.12/site-pmâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m \u001b[33m0:00:02\u001b[0m\n",
      "ackages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees\n",
      "of freedom is <= 0. Correction should be strictly less than the reduction factor\n",
      "(input numel divided by output numel). (Triggered internally at \n",
      "../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\u001b[2KğŸ¤— \u001b[33mSegmenting...\u001b[0m \u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m \u001b[33m0:00:14\u001b[0m\n",
      "\u001b[?25hVoila!âœ¨ Your file has been transcribed & speaker segmented go check it out over here ğŸ‘‰ /work/speech/Kasper/PrÃ¦test/P4-transcription.json\n",
      "file 2 of 2 file=/work/speech/Kasper/PrÃ¦test/P9.WMA\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n",
      "\u001b[2KğŸ¤— \u001b[33mTranscribing...\u001b[0m \u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[33m0:00:00\u001b[0mUsing `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
      "\u001b[2KğŸ¤— \u001b[33mTranscribing...\u001b[0m \u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[33m0:01:12\u001b[0m\n",
      "\u001b[?25h/home/ucloud/.local/share/pipx/venvs/insanely-fast-whisper/lib/python3.12/site-packages/lightning_fabric/utilities/cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[2K/home/ucloud/.local/share/pipx/venvs/insanely-fast-whisper/lib/python3.12/site-pmâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "ackages/insanely_fast_whisper/utils/diarize.py:55: UserWarning: The given NumPy \n",
      "array is not writable, and PyTorch does not support non-writable tensors. This \n",
      "means writing to this tensor will result in undefined behavior. You may want to \n",
      "copy the array to protect its data or make it writable before converting it to a\n",
      "tensor. This type of warning will be suppressed for the rest of this program. \n",
      "(Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  diarizer_inputs = torch.from_numpy(inputs).float()\n",
      "\u001b[2K/home/ucloud/.local/share/pipx/venvs/insanely-fast-whisper/lib/python3.12/site-p0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "ackages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: \n",
      "TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility \n",
      "issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n",
      "\u001b[2K/home/ucloud/.local/share/pipx/venvs/insanely-fast-whisper/lib/python3.12/site-pmâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "ackages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees\n",
      "of freedom is <= 0. Correction should be strictly less than the reduction factor\n",
      "(input numel divided by output numel). (Triggered internally at \n",
      "../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\u001b[2KğŸ¤— \u001b[33mSegmenting...\u001b[0m \u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[37mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m\u001b[93mâ”\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
      "\u001b[?25hVoila!âœ¨ Your file has been transcribed & speaker segmented go check it out over here ğŸ‘‰ /work/speech/Kasper/PrÃ¦test/P9-transcription.json\n"
     ]
    }
   ],
   "source": [
    "for idx, file in enumerate(files):\n",
    "    print(f\"file {idx+1} of {length} file={file}\")\n",
    "    transcribe(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1c6c9-7fff-4e6e-87a5-e5c4eb5b88c9",
   "metadata": {},
   "source": [
    "### 3. Convert transcriptions to Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899ce9f-a4e8-41a2-8f9f-820256f192ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following 2 files will be converted to docx ['/work/speech/Kasper/PrÃ¦test/P9-transcription.json', '/work/speech/Kasper/PrÃ¦test/P4-transcription.json']\n",
      "file 1 of 2 file=/work/speech/Kasper/PrÃ¦test/P9-transcription.json\n",
      "file 2 of 2 file=/work/speech/Kasper/PrÃ¦test/P4-transcription.json\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "from docx import Document\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "\n",
    "\n",
    "def diarization_to_docx_edit(audio_file_diarization):\n",
    "\n",
    "    audio_file_basename = os.path.basename(audio_file_diarization)\n",
    "    \n",
    "    doc = Document()\n",
    "    \n",
    "    paragraph = doc.add_paragraph()\n",
    "    paragraph.add_run(audio_file_basename).bold = True\n",
    "    \n",
    "    paragraph_format = paragraph.paragraph_format\n",
    "    paragraph_format.alignment\n",
    "    paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    \n",
    "    f = open(audio_file_diarization, 'r', encoding=\"utf-8\")\n",
    "\n",
    "    data = json.load(f)\n",
    "\n",
    "    for entry in data[\"speakers\"]:\n",
    "        paragraph = doc.add_paragraph()\n",
    "        paragraph.add_run(str(entry[\"timestamp\"]) + \"\\n\")\n",
    "        paragraph.add_run(str(entry[\"speaker\"])  + \"\\n\")\n",
    "        paragraph.add_run(str(entry[\"text\"]).strip())\n",
    "\n",
    "            \n",
    "    docx_filename = audio_file_diarization.replace(\".json\", \"_edit.docx\")\n",
    "    doc.save(docx_filename)\n",
    "\n",
    "\n",
    "# absolute path containing the json diarization files\n",
    "extension = \"json\"\n",
    "path_json_regex = r'' + re.escape(path_audio) + '*.' + re.escape(extension)\n",
    "files_json = glob.glob(path_json_regex)\n",
    "length_json = len(files_json)\n",
    "\n",
    "print(f\"The following {length_json} files will be converted to docx {files_json}\")\n",
    "\n",
    "for idx, file in enumerate(files_json):\n",
    "    print(f\"file {idx+1} of {length} file={file}\")\n",
    "    diarization_to_docx_edit(file)\n",
    "print(f\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
